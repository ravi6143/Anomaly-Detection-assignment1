{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06937aac-2658-4d2e-a982-a84296bf3928",
   "metadata": {},
   "source": [
    "# Question - 1\n",
    "ans - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce78a86-8ca7-4efc-a204-d0ac3f6ca437",
   "metadata": {},
   "source": [
    "Anomaly detection is a technique used in data analysis to identify patterns or instances that deviate significantly from the expected behavior within a dataset. Its purpose is to pinpoint unusual observations or outliers that may indicate interesting events, errors, or potential threats in various applications.\n",
    "\n",
    "# The main objectives of anomaly detection are:\n",
    "\n",
    "1. Identifying Novel Patterns: Anomaly detection helps in discovering previously unknown patterns or behaviors within a dataset that may not conform to typical expectations.\n",
    "\n",
    "2. Highlighting Suspicious Events: It flags data points or events that are significantly different from the majority of the dataset, helping to identify potential anomalies or anomalies that require further investigation.\n",
    "\n",
    "3. Improving Data Quality: By identifying outliers or errors in the data, anomaly detection can contribute to improving data quality and reliability.\n",
    "\n",
    "4. Supporting Decision Making: Anomaly detection provides valuable insights that can aid decision-making processes, such as fraud detection, network security, fault detection in machinery, and healthcare monitoring\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a8c388-9cd8-4a57-ac6f-b41b1cce2b87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e173eeb-f15c-4056-bb9b-a8433f375eed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ab6c21-9bf9-4077-8eab-dc2519c43a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acebb45-5476-486f-9747-0833b9f573fe",
   "metadata": {},
   "source": [
    "# Question - 2\n",
    "ans - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04fb51f-aed2-409e-9312-efff90a3511e",
   "metadata": {},
   "source": [
    "Anomaly detection poses several challenges due to the diverse nature of data and the complexity of identifying outliers.\n",
    "\n",
    "Some key challenges include:\n",
    "\n",
    "1. Unlabeled Data: \n",
    "\n",
    "In many real-world scenarios, labeled anomalies may be scarce or entirely absent, making it difficult to train supervised anomaly detection models. This necessitates the use of unsupervised or semi-supervised techniques, which can be less accurate or require more sophisticated algorithms.\n",
    "\n",
    "2. Imbalanced Data: \n",
    "\n",
    "Anomalies are often rare events compared to normal instances, leading to imbalanced datasets where the number of normal data points far exceeds the number of anomalies. This can lead to biased models that favor normal instances and overlook anomalies.\n",
    "\n",
    "3. Data Quality Issues: \n",
    "\n",
    "Anomalies can sometimes be caused by errors or noise in the data, making it challenging to distinguish true anomalies from data artifacts. Preprocessing techniques and robust anomaly detection algorithms are necessary to address data quality issues.\n",
    "\n",
    "4. Scalability: \n",
    "\n",
    "Anomaly detection algorithms must be capable of handling large-scale datasets efficiently. As the volume of data grows, scalability becomes a significant challenge, requiring algorithms that can process data in parallel or in streaming fashion.\n",
    "\n",
    "5. High-Dimensional Data: \n",
    "\n",
    "In high-dimensional datasets, distinguishing between normal and anomalous patterns becomes increasingly difficult due to the curse of dimensionality. Dimensionality reduction techniques and specialized anomaly detection algorithms for high-dimensional data are needed to address this challenge.\n",
    "\n",
    "6. Concept Drift: \n",
    "\n",
    "In dynamic environments, the characteristics of normal and anomalous behavior may change over time, leading to concept drift. Anomaly detection models must be adaptive to evolving patterns and capable of detecting changes in the data distribution.\n",
    "\n",
    "7. Interpretability: \n",
    "\n",
    "Many anomaly detection algorithms produce black-box models that lack interpretability, making it challenging to understand why certain data points are flagged as anomalies. Explainable anomaly detection methods are needed for applications where interpretability is essential.\n",
    "\n",
    "8. Anomaly Definition: \n",
    "\n",
    "Defining what constitutes an anomaly can be subjective and context-dependent. Anomalies may vary in nature across different domains, requiring flexible anomaly detection techniques that can adapt to diverse definitions of anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d719d8f-28d4-4056-834e-59929af994e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54869ee4-55f7-4d18-8510-33bdbd45add4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8eb4e6-bfd6-41a3-848d-5c2d94076df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447d02a5-097b-4699-90e8-e9efbbaa6a1f",
   "metadata": {},
   "source": [
    "# Question - 3\n",
    "ans - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595db5d6-5bf1-431a-8083-2da40a5b7655",
   "metadata": {},
   "source": [
    "Unsupervised anomaly detection and supervised anomaly detection are two distinct approaches to identifying anomalies within a dataset, differing primarily in their use of labeled data for training. Here's how they differ:\n",
    "\n",
    "# Unsupervised Anomaly Detection:\n",
    "\n",
    "*  Training: In unsupervised anomaly detection, the algorithm learns the normal patterns or structure of the data without the use of labeled anomalies. It operates solely on the input data and does not require any prior knowledge of anomalies.\n",
    "\n",
    "*  Anomaly Detection: Unsupervised methods identify anomalies as data points or patterns that deviate significantly from the expected behavior of the majority of the data. Anomalies are detected based on their deviation from the normal distribution or clustering patterns within the data.\n",
    "\n",
    "* Examples: Density-based methods like Local Outlier Factor (LOF), distance-based methods like k-nearest neighbors (KNN), and clustering techniques like DBSCAN are commonly used in unsupervised anomaly detection.\n",
    "\n",
    "# Supervised Anomaly Detection:\n",
    "\n",
    "* Training: Supervised anomaly detection algorithms require labeled data, where anomalies are explicitly identified and labeled during the training phase. The algorithm learns to distinguish between normal and anomalous instances based on these labels.\n",
    "\n",
    "\n",
    "* Anomaly Detection: During the testing phase, the supervised model predicts whether new instances are normal or anomalous based on the learned patterns from the labeled training data. The model assigns anomaly scores or probabilities to data points, which are used to determine their anomaly status.\n",
    "\n",
    "* Examples: Supervised anomaly detection methods include classification algorithms like Support Vector Machines (SVM), decision trees, or neural networks trained with anomaly labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087d32ad-09e4-4937-84eb-0eca69a692b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1593fb21-fe82-416c-b840-0ed734050842",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97d8a8e-2fef-41e2-8a08-0444e72ea6c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7f8ad1e1-83ef-4998-b464-4bffaa9d3bc0",
   "metadata": {},
   "source": [
    "# Question - 4\n",
    "ans - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd05cc9b-e496-4699-a60e-0fb4f9726a5b",
   "metadata": {},
   "source": [
    "# 1 Statistical Methods:\n",
    "\n",
    "* These algorithms rely on statistical properties of the data to identify anomalies. They include techniques such as:\n",
    "\n",
    "* Z-Score or Standard Score\n",
    "\n",
    "* Grubbs' Test\n",
    "\n",
    "* Hampel Filter\n",
    "\n",
    "\n",
    "# 2 Machine Learning-Based Methods:\n",
    "\n",
    "* These algorithms use machine learning techniques to model normal behavior and detect deviations indicative of anomalies. They include:\n",
    "\n",
    "* Supervised Learning (e.g., classification algorithms)\n",
    "\n",
    "* Unsupervised Learning (e.g., clustering, density estimation)\n",
    "\n",
    "* Semi-Supervised Learning\n",
    "\n",
    "\n",
    "# 3 Density-Based Methods:\n",
    "\n",
    "* These algorithms focus on estimating the density of data points and identifying outliers as those with significantly lower density. Examples include:\n",
    "\n",
    "* Local Outlier Factor (LOF)\n",
    "\n",
    "* DBSCAN (Density-Based Spatial Clustering of Applications with Noise)\n",
    "\n",
    "# 4 Proximity-Based Methods:\n",
    "\n",
    "* These algorithms measure the distance or similarity between data points and identify anomalies based on their proximity to other points. Examples include:\n",
    "\n",
    "* K-Nearest Neighbors (KNN)\n",
    "\n",
    "* One-Class SVM (Support Vector Machine)\n",
    "\n",
    "# 5 Model-Based Methods:\n",
    "\n",
    "* These algorithms involve fitting a statistical or machine learning model to the data and identifying anomalies based on deviations from the model's predictions. Examples include:\n",
    "\n",
    "* Gaussian Mixture Models (GMM)\n",
    "\n",
    "* Autoencoders\n",
    "\n",
    "# 5 Time Series Methods:\n",
    "\n",
    "* These algorithms are specialized for detecting anomalies in time series data. They include techniques such as:\n",
    "\n",
    "* Seasonal Decomposition\n",
    "\n",
    "* Exponential Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e6ef47-62b8-4715-9458-1bec0eac5e32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c48d85-b883-4db8-b8ce-c94c13fd8025",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2afa592-9120-42c9-9f3b-64ba95205d7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4e37bb2b-98da-4b64-bf54-1652fa715679",
   "metadata": {},
   "source": [
    "# Question - 5\n",
    "ans - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec35e64-b254-4784-b671-acf14acd2f57",
   "metadata": {},
   "source": [
    "Distance-based anomaly detection methods rely on certain assumptions about the distribution and characteristics of normal data. The main assumptions made by these methods include:\n",
    "\n",
    "1. Normal Data Clustering: Distance-based methods assume that normal data points tend to cluster together in the feature space. This means that most of the data points are relatively close to each other, forming dense clusters, while anomalies are isolated and distant from the majority of the data.\n",
    "\n",
    "2. Local Density Variation: These methods assume that the density of data points varies across different regions of the feature space. In dense regions, the distance between neighboring data points is small, while in sparse regions, the distance is larger. Anomalies are expected to occur in low-density regions.\n",
    "\n",
    "3. Outlier Separability: Distance-based methods assume that anomalies are sufficiently distant from normal data points and can be separated from them based on distance measures. Anomalies are typically considered as data points that lie in regions with sparse data or have unusually large distances from their nearest neighbors.\n",
    "\n",
    "4. Euclidean Distance Metric: Many distance-based anomaly detection methods assume the use of the Euclidean distance metric to measure distances between data points. This assumes that the features are numeric and continuous, and that the Euclidean distance accurately captures the similarity or dissimilarity between data points.\n",
    "\n",
    "5. Robustness to Noise: Distance-based methods assume some level of robustness to noise or small perturbations in the data. However, excessive noise or outliers may degrade the performance of these methods, as they can distort distance measurements and lead to false anomaly detections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77621a68-188c-4487-a272-04ba7137512e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22baf9b6-c745-4a93-8442-9ffb4aa0e85b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb63664c-505c-4408-8fda-3a8aa96cd873",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136350f5-4cbc-455b-8c26-aa348cfbfa7c",
   "metadata": {},
   "source": [
    "# Question - 6\n",
    "ans - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac19318-9b3d-4df3-8588-4b840e01a98b",
   "metadata": {},
   "source": [
    "The Local Outlier Factor (LOF) algorithm computes anomaly scores for each data point by comparing its local density to that of its neighbors. Here's an overview of how LOF calculates anomaly scores:\n",
    "\n",
    "# 1 Local Density Estimation:\n",
    "\n",
    "For each data point xi, LOF calculates its local density based on the distance to its k nearest neighbors. The local density \n",
    "density(xi) is inversely proportional to the average distance to its neighbors. Higher density implies that the point is in a denser region and lower density implies it's in a sparse region.\n",
    "\n",
    "\n",
    "# 2 Reachability Distance:\n",
    "\n",
    "LOF computes the reachability distance reachdist reachdist(xi,xj) between each data point xi and its neighbor xj. The reachability distance measures how far xi is from its neighbor xj in terms of local density. It is defined as the maximum of the distance between xi and xjand the density of xj .\n",
    "\n",
    "\n",
    "\n",
    "# 3 Local Reachability Density:\n",
    "\n",
    "For each data point xi , LOF calculates its local reachability density Lrd(xi) as the inverse of the average reachability distance of its neighbors. This measure reflects the average reachability of xi \n",
    "\n",
    "with respect to its local neighborhood.\n",
    "\n",
    "\n",
    "# 4 Local Outlier Factor (LOF):\n",
    "\n",
    "Finally, LOF computes the anomaly score (LOF(xi)) for each data point xi by comparing its local reachability density to that of its neighbors. It is defined as the ratio of the average local reachability density of xi's neighbors to its own local reachability density. A value greater than 1 indicates that xi has a lower density compared to its neighbors, making it an outlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64b7bf2-43ff-4938-9297-520fb0c1315d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08292025-d97e-45d4-b44e-2910c89181eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c05bda5e-ffad-45ae-9cb5-8bb2ab199602",
   "metadata": {},
   "source": [
    "# Question - 7\n",
    "ans - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7966155b-6e82-4eba-bef3-26672dd46bca",
   "metadata": {},
   "source": [
    "# 1 Number of Trees (n_estimators):\n",
    "\n",
    "This parameter determines the number of isolation trees to be built. A higher number of trees can lead to better performance but may increase computation time. Typically, increasing the number of trees improves the accuracy of the anomaly detection process.\n",
    "\n",
    "\n",
    "#  2 Subsample Size (max_samples):\n",
    "It specifies the number of samples to be drawn from the dataset to construct each isolation tree. A smaller subsample size can speed up the training process, but it may lead to less accurate results. The default value is often set to the size of the dataset.\n",
    "\n",
    "\n",
    "# 3 Maximum Tree Depth (max_depth):\n",
    "\n",
    "This parameter controls the maximum depth of each isolation tree in the forest. A deeper tree can capture more complex relationships in the data but may also lead to overfitting. Limiting the maximum tree depth helps prevent overfitting and improves generalization.\n",
    "\n",
    "\n",
    "# 4 Contamination:\n",
    "\n",
    "The contamination parameter specifies the expected proportion of anomalies in the dataset. It is used to set a threshold for identifying outliers. Anomalies with anomaly scores higher than the contamination value are flagged as outliers. If not explicitly provided, Isolation Forest estimates the contamination based on the assumption that outliers are rare.\n",
    "\n",
    "\n",
    "# 5 Bootstrap Sampling (bootstrap):\n",
    "\n",
    "This boolean parameter controls whether bootstrap sampling is used to draw samples with replacement when building each isolation tree. Bootstrapping can introduce diversity among the trees and improve the robustness of the model.\n",
    "\n",
    "\n",
    "# 6 Random Seed (random_state):\n",
    "\n",
    "This parameter specifies the random seed used for random number generation. Setting a random seed ensures reproducibility of results across multiple runs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752c1823-7fc8-463e-8464-26fa93f24c33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdffa2f1-2ab9-4faa-8e14-106d0cb56cb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ca780a68-0e17-4318-8dd8-d4d5cd278a0c",
   "metadata": {},
   "source": [
    "# Question - 8\n",
    "ans - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81739b6f-8dee-4f44-b360-0fdb7eaf80be",
   "metadata": {},
   "source": [
    "The anomaly score (ASx) for x can be calculated as:\n",
    "\n",
    "# ASx = 1- number of neighbors of the same class / k \n",
    "\n",
    "\n",
    "\n",
    "since k = 10  and neigbors are 2\n",
    "\n",
    "ASx = 1- (2)/10\n",
    "\n",
    "# Anomaly socre will be 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608a9ee2-63e3-40b6-94c3-ec7871954343",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b39b88-9785-4e26-bb46-d6a14ad201af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8405296a-b878-46c9-82e9-2a50ec1f9e60",
   "metadata": {},
   "source": [
    "# Question - 9\n",
    "ans - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e269f05-5e4e-47a6-8964-3a22d17d39fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "\n",
    "x,y = make_classification(n_features=2, n_samples=3000,n_informative=2,\n",
    "                           n_redundant=0, n_clusters_per_class=1, random_state=42)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "df = pd.DataFrame(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6564b6e-7972-42ea-a4a1-621bb0aafbe6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>IsolationForest(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">IsolationForest</label><div class=\"sk-toggleable__content\"><pre>IsolationForest(random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "IsolationForest(random_state=42)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "clf = IsolationForest(n_estimators=100 ,random_state=42)\n",
    "\n",
    "clf.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b35fb69-85e3-4e81-a887-87e260ff2580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomaly Score: 2.7120775510764933e-49\n"
     ]
    }
   ],
   "source": [
    "average_path_length = 5\n",
    "\n",
    "\n",
    "\n",
    "anomaly_score = 2 ** (-(average_path_length) / clf.decision_function(df).mean())\n",
    "\n",
    "print(\"Anomaly Score:\", anomaly_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8745b74d-825b-4f30-b699-3d48962e875a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
